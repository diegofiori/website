# Software 2.1
** Note: this article has been written the 16th of April 2023. The field of AI is evolving at a fast pace and the some content of this article may be outdated. **

Few notes on the evolution of software. Inspired by Karpathy’s article on Medium “Software 2.0”.

**Software 1.0** is what we classically define as software, i.e. the lines of code a programmer writes in his favorite programming language. It is still quite used in all the verticals of programming, however, it is often over-performed by pieces of software 2.0, the neural nets. 

**Software 2.0** is a term coined by *Andrej Karpathy* in 2017, when he was analysing the new paradigm of coding we were shifting to thanks to Neural Networks. Instead of writing lines of code in Python, programming in the Software 2.0 era means to label data, select a model architecture and find the optimal model using an optimization algorithm on the architecture-defined search space.


**Software 2.1** is the natural evolution of what Karpathy exposed 5 years ago, but updated with the current state of AI research. The explosion of neural network size and the ability of networks trained on generic data to be effortlessly applied to specific tasks has created a new approach of programming the Software 2.0 computers: the Transformers. Karpathy’s idea of programming through labelling tons of data data, choose model architectures  and train it from scratch has shifted to manually label few data, choose a Large Model and either fine-tune or use prompt engineering on top of it. The paradigm has shifted from training NN from scratch to adapt large generative models to the needed use cases.

# A new paradigm

Neural Networks with few million of parameters have been replaced by huge Transformers with billions (if not trillions) of parameters. These models have been trained in generic environments, and usually are not specialized in a single task. On the other hand, they can be either fine-tuned or applied with a technique called few-shot learning directly to the new task, with no extra training. The large models are usually data-efficient, i.e. they can learn new tasks needing far less data respect to training a much smaller model from scratch. Furthermore, new efficient and small models can be extracted from the large ones using techniques as Knowledge Distillation and post-training quantization (SmoothQuant). In this way, the Large Model (possibly fine-tuned) can be used as teacher for a much smaller model which will just learn on tons of un-labeled data to mimic the teacher prediction. In this way, just the topic-related knowledge is extracted from the huge model.

Thus, there is a new paradigm change in the Software 2.0 era. Instead of labelling tons of data by hand, the Software 2.1 programmer can just label few data per class, then pick one of the so called “Foundation Models”, fine tune it and / or reduce it. Eventually, he can use the final model exactly for solving the specific task. As you can see the paradigm is almost the same of Software 2.0 since the “code” is encoded in the networks parameters. However, the way we use and train the model can be quite different respect to a Software 2.0 approach. Techniques as p-tuning, prompt engineering and few-shots learning allow the programmer to adapt generic large models to smaller tasks without the need of modifying the weights. 

This trend is getting more and more clear since in order to reach AGI, researchers are building larger and larger models feeding them with more and more data for getting better and better results (and Transformers seem damn good for the job). In particular, in the last year research got quite interested in the so called multimodal model as DeepMind’s GATO, which essentially are models fed with billion of tokens which are trained not with text, but with also images and videos. In this way the model can developed different skills and have broader intuition of the world. Furthermore, the model has a far more general comprehension of the world and hypothetically can be used for narrow tasks with a bit of fine-tuning or prompt-engineering. 

> “Gradient descent can write code better than you. I’m sorry.” 
A. Karpathy on Twitter
> 

Software 2.0 was basically equivalent to say that “the code is in practice written by the optimization process”, i.e. a gradient descent based optimizer. In software 2.1 this is still true, but the search space is not constrained by the model architecture, but just from the knowledge and the generalization ability of the large model. Note that assuming enough data, the Software 2.0 approach can still give optimal results, but as Codex has shown, fine-tuning a Large Model can easily give the same (if not better) results with much less labeled data. Let’s focus for a while on the difference here: instead of letting the gradient descent to directly “write the code for you”, you take a huge model, trained with a different scope and use prompt engineering or fine-tuning getting the optimal model. So, using Karpathy’s analogy, we can say that in the first case the code was already in the weights and it is extracted using the correct prompt, while in the second case it is still the gradient descent doing the job, adapting the “code” in the weights to a better version of it. We can philosophically imagine the gradient descent being like the “software engineer” taking a code poorly written for a task and fix it and transform it into an efficient algorithm ready for production. On the other hand,  using knowledge distillation for reducing the model size and extracting just the information relevant to the task, it can be interpreted as giving a slow Python code to the “optimization engineer” and let him translate the slow code into a faster implementation in C++ or CUDA.

# Pro and Cons

In this part I would not cover the advantages and disadvantages respect to Software 1.0 since it would just be a repetition of what Karpathy said in his 2017 article. Instead, I would focus on the differences respect to the Software 2.0 approach.

## Advantages

- Few data-sample per class needed. Instead of needing to label tons of data and training a neural net from scratch, it is just sufficient to manually label few data per class and adapt a large general model to the desired task.
- Quick 0 to 1. Building a prototype using the prompt-engineering approach takes much less time than training a model from scratch.
- We can build an efficient and deployable model simply distilling the task-specific knowledge out of the large model.
- The generality of the large model can be exploited for running multiple tasks on top of the same model.
- Fine-tuned large models can reach top-notch performance in specific tasks. The performance obtained with these models cannot be easily reached with 2.0 approach if not with labelling an infinite amount of data.

## Limitations

Software 2.1 has approximately the same limitations of software 2.0. Moreover, software 2.1 falls back to the 2.0 approach when there exists no model which can be fine-tuned for covering the needed use-case. In fact, if no large model has an input data type compatible with the needed task or the task complexity is such that the large model has no knowledge about it, it is necessary to build a new model and train it directly on the task itself. 

Another limitation is the size of the Large Foundation Models. Deploying a model with hundred of billions of parameters is not an easy task and needs a lot of engineering effort. A model of this dimension cannot be loaded on a single GPU (not even the brand new H100) and it is necessary to split it on multiple devices. 

Inference cost and latency are also problems when dealing with huge models. A possible solution is to distill all the task-specific information out of the large model into a much smaller version of it. This approach is technically similar to the one adopted in Software 2.0, but it doesn’t need a large amount of labeled data. In fact the large model predictions can also be learned by the student model on tons of un-labeled data. Furthermore, foundation models come with different available size, so the programmer can simply choose a smaller version of it when latency and cost limit are not satisfied by larger versions.

## Co-existence

The programming world is evolving in what karpathy predicted in his article “Software 2.0”. AI models are getting state-of-the-art results in multiple verticals. The hand written software 1.0 code is being replaced by AI models, improving the existing 1.0 state of the art of multiple order of magnitude. Neural networks are proving their efficiency and performance on many verticals and with the arrival of huge models a new paradigm rise: AI models can even generate software 1.0 code. OpenAI’s Codex and the associated GitHub Copilot are the perfect example, showing the terrific possibilities of large models. Let’s focus on the last implication for a while: AI is proving its power in writing performant code in the networks weights and at the same time AI models are even used for writing part of the code-base needed for models deployment. It is obvious that the future is going in the direction of co-existence between AI and Software 1.0. However, a huge part of the 1.0 software will probably be generated by AI too. But AI is not only Software 2.0 anymore. Huge models allow a totally different approach to the field enabling what we called Software 2.1. The raise of new more capable foundational model will allow the AI programmers to train powerful models with fewer data and at the same time get higher quality results than what they could have achieved using just the software 1.0 approach.

## How we program at Nebuly

At Nebuly we embrace the co-existence of the three approaches. Since we are thrived by efficiency we always look for paths leading to optimization and we recognize that in multiple verticals AI has shown super human programming capabilities. Computer vision and NLP are the most obvious example: no programmer in the world would be good enough for crafting a software 1.0 algorithm able to over-perform an AI model. The same is true for text generation in NLP. With these facts in mind we think about applying similar paradigms to our daily job: make AI efficient. Clearly a huge part of the job is still done with a Software 1.0 approach, however, every time we design an optimization algorithm we should always keep in mind that it is critical for us to go over human performance. This means that each system must be designed with a software 2.0/2.1 long term vision. It is necessary to collect data and study the large transformers model in order to switch as soon as possible to the 2.x paradigm, and the collection must clearly be performed using a software 1.0 approach. But even our 1.0 approach is actually AI supported: at nebuly we are huge fans of the GitHub Co-Pilot and all our products have been implemented using this terrific tool. 

Our goal is to make AI efficient and the only way we can achieve it is to reach a super-human performance in code-optimization. This result can only be obtained using AI for optimizing AI and we are building our framework for getting there.